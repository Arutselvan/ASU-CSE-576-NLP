{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "com_qa_task_instances",
      "provenance": [],
      "authorship_tag": "ABX9TyOcX7ThkNWRwtApf3tQK0LN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arutselvan/ASU-CSE-576-NLP/blob/main/HW_2/com_qa_task_instances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mma_COmnFI8F",
        "outputId": "6d182623-42e1-40fa-d860-b879c4c33bbe"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.11.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.4)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.16)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqev2MxGFkKE",
        "outputId": "add1a931-b3e3-4786-d16f-19708a7383f5"
      },
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "dataset = load_dataset(\"com_qa\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset com_qa (/root/.cache/huggingface/datasets/com_qa/default/0.1.0/3f5b9b53f3ff9c3423018530911f2031542f6a8b66c6709f69475ab27e43df63)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED2XYcxkFvMs",
        "outputId": "8d86da54-0889-44c9-c0c8-a03e68ef2cef"
      },
      "source": [
        "merged_dataset = concatenate_datasets([dataset['train'],dataset['test'],dataset['validation']])\n",
        "len(merged_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7175"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlF3KU9CH9b4",
        "outputId": "e5f7ec1c-b848-4c69-f9d9-43c4a7554f35"
      },
      "source": [
        "rephrase_dataset = merged_dataset.filter(lambda x: len(x['questions'])>1)\n",
        "\n",
        "rephrase_instances = []\n",
        "\n",
        "for cluster in rephrase_dataset:\n",
        "  rephrase_instances.append({\n",
        "      \"input\": \"Question: \" + cluster['questions'][0],\n",
        "      \"output\": cluster['questions'][1:]\n",
        "  })\n",
        "\n",
        "len(rephrase_instances)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/com_qa/default/0.1.0/3f5b9b53f3ff9c3423018530911f2031542f6a8b66c6709f69475ab27e43df63/cache-e6244efb342920df.arrow\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1805"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "togNjVZoJsCs",
        "outputId": "12b418b0-f24b-4ce8-b3dd-3f90bf9fbbca"
      },
      "source": [
        "wikipedia_dataset = merged_dataset.filter(lambda x: \"https://en.wikipedia.org\" in x['answers'][0])\n",
        "\n",
        "topic_questions_instances = []\n",
        "\n",
        "for wikians in wikipedia_dataset:\n",
        "  topic_questions_instances.append({\n",
        "      \"input\": \"Answer: \" + wikians['answers'][0].split('/')[-1].replace('_',' '),\n",
        "      \"output\": wikians['questions']\n",
        "  })\n",
        "\n",
        "len(topic_questions_instances)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/com_qa/default/0.1.0/3f5b9b53f3ff9c3423018530911f2031542f6a8b66c6709f69475ab27e43df63/cache-3b959d55eaebf71f.arrow\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4982"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK_AJyh3OIZv"
      },
      "source": [
        "wikilink_questions_instances = []\n",
        "\n",
        "for wikilink in wikipedia_dataset:\n",
        "  wikilink_questions_instances.append({\n",
        "      \"input\": \"Questions: \" + str(wikilink['questions']),\n",
        "      \"output\": [wikilink['answers'][0]]\n",
        "  })\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGNEjNgrMuEw",
        "outputId": "76932f6d-fc12-4a72-cd4c-846dcba59614"
      },
      "source": [
        "answer_dataset = merged_dataset.filter(lambda x: len(x['answers'])==1 and \"wikipedia\" not in x['answers'][0] )\n",
        "\n",
        "direct_answer_questions = []\n",
        "\n",
        "for answer in answer_dataset:\n",
        "  direct_answer_questions.append({\n",
        "      \"input\": \"Questions: \" + str(answer['questions']),\n",
        "      \"output\": [answer['answers'][0]]\n",
        "  })\n",
        "\n",
        "direct_answer_questions = [x for x in direct_answer_questions if x['output']!=[\"\"] or x['output']!=['']]\n",
        "\n",
        "len(direct_answer_questions)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/com_qa/default/0.1.0/3f5b9b53f3ff9c3423018530911f2031542f6a8b66c6709f69475ab27e43df63/cache-fe0fb4777c4b6ff7.arrow\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1574"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MowU4v0JgqM"
      },
      "source": [
        "import json\n",
        "\n",
        "CONTRIBUTOR = \"Arut Selvan Dhanasekaran\"\n",
        "SOURCE = \"opus_paracrawl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax6Y1NMYwd1n"
      },
      "source": [
        "# Task 1\n",
        "\n",
        "definition_1 = \"Given a question, create questions that are rephrasals of the original question. Additional contextual information if available can be used to frame the questions. However answer for all the generated questions should be same and should match the answer of the original question.\"\n",
        "\n",
        "positive_examples_1 = [{\n",
        "    \"input\": \"Question: what places in africa do people speak french?\",\n",
        "    \"output\": '[\"where in africa do people speak french?\", \"what countries in africa speak french?\", \"where in africa do people speak french?\"]',\n",
        "    \"explanation\": \"The generated questions mean the same as the input question and the answer will be the same for all the questions\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Question: hitler became chancellor of germany in what year?\",\n",
        "    \"output\": '[\"what year did hitler become chancellor of germany?\", \"what year was hitler elected the chancellor of germany?\", \"in which year did hitler become the chancellor of germany?\", \"what year did adolf hitler become chancellor for germany?\", \"which date did hitler become chancellor of germany?\"]',\n",
        "    \"explanation\": \"The answer for all the generated questions and the given question can be given by a single date\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Question: mandeville lousiana zip code?\",\n",
        "    \"output\": '[\"what is the zip code for mandeville louisiana?\", \"what is mandeville louisiana 9 digit zip code?\"]',\n",
        "    \"explanation\": \"The rephrased questions exactly means the same as the given question even though some additional contextual information was used in one of the questions created.\"\n",
        "}]\n",
        "\n",
        "negative_examples_1 = [{\n",
        "    \"input\": \"Question: How long is the Nile river?\",\n",
        "    \"output\": '[\"Is Nile the longest river?\"]',\n",
        "    \"explanation\": \"The question created changes the meaning of the input question and the answer is not the same for the questions\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Question: Who is the first football player to win seven balon d'ors?\",\n",
        "    \"output\": '[\"Who won the first balon dor?\", \"who is the first football player?\"]',\n",
        "    \"explanation\": \"The generated questions don't mean the same as the original questions and the answers are different for each question.\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Question: How Vitamin C helps the human body?\",\n",
        "    \"output\": '[\"How vitamins help humans?\", \"Does Vitamin C help the human body?\"]',\n",
        "    \"explanation\": \"One of the questions created generalises a specific question. The other changes the meaning of the given question.\"\n",
        "}]\n",
        "\n",
        "temp_set = set()\n",
        "rephrase_instances_cleaned = []\n",
        "for instance in rephrase_instances:\n",
        "  if instance['input'] not in temp_set:\n",
        "    temp_set.add(instance['input'])\n",
        "    rephrase_instances_cleaned.append(instance)\n",
        "\n",
        "  \n",
        "\n",
        "task_1 = {\n",
        "    \"Contributors\": [CONTRIBUTOR],\n",
        "    \"Source\": [SOURCE],\n",
        "    \"Categories\": [\"Question Generation\"],\n",
        "    \"Definition\": definition_1,\n",
        "    \"Positive Examples\": positive_examples_1,\n",
        "    \"Negative Examples\": negative_examples_1,\n",
        "    \"Instances\": rephrase_instances_cleaned\n",
        "}\n",
        "\n",
        "with open(\"task145_com_qa_question_generation.json\", 'w') as fout:\n",
        "    json_dumps_str = json.dumps(task_1, indent=4)\n",
        "    print(json_dumps_str, file=fout)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCwBP2ojzcip"
      },
      "source": [
        "# Task 2\n",
        "\n",
        "definition_2 = \"An answer is given. Generate questions directly related to that answer. Don't create questions on topics indirectly related to the given answer. The questions generated should be directly or partially answered by the given answer.\"\n",
        "\n",
        "positive_examples_2 = [{\n",
        "    \"input\": \"Answer: victoria woodhull\",\n",
        "    \"output\": '[\"who was the first women to run for presidency in the us?\", \"who was the first women to run for the us president?\", \"which women was the first to run for president in the us?\", \"first women to run for presidency of the usa?\"]',\n",
        "    \"explanation\": \"The given answer directly answers all the generated questions\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Answer: atlanta\",\n",
        "    \"output\": '[\"Top 3 largest citites in Georgia?\"]',\n",
        "    \"explanation\": \"Although the answer doesn't directly answer the question, the given answer is a partial answer to the generated question and relates directly to it.\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Answer: united states presidential election, 1860\",\n",
        "    \"output\": '[\"when was abraham lincoln voted for president?\", \"when was president lincoln voted president?\", \"when was lincoln president?\"]',\n",
        "    \"explanation\": \"The questions generated can be answered by the given answer\"\n",
        "}]\n",
        "\n",
        "negative_examples_2 = [{\n",
        "    \"input\": \"Answer: Infrared rays\",\n",
        "    \"output\": '[\"X rays are in which part of the electromagnetic spectrum?\"]',\n",
        "    \"explanation\": \"The question and the answer is a part of a broader topic but they're not directly related\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Answer: Coca Cola\",\n",
        "    \"output\": '[\"Are soft drinks harmful to health?\", \"Is Pepsi better?\"]',\n",
        "    \"explanation\": \"The questions are indirectly related to the given answer but in no way the given answer directly or indirectly answers the generated questions\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Answer: Martin Luther King\",\n",
        "    \"output\": '[\"Who is the first African-American president of USA?\", \"How prevalent was racial inequality in the 2000s?\"]',\n",
        "    \"explanation\": \"The answer and questions are not directly related and the answer doesn't even indirectly answer the questions generated\"\n",
        "}]\n",
        "\n",
        "temp_set = set()\n",
        "topic_questions_instances_cleaned = []\n",
        "for instance in topic_questions_instances:\n",
        "  if instance['input'] not in temp_set:\n",
        "    temp_set.add(instance['input'])\n",
        "    topic_questions_instances_cleaned.append(instance)\n",
        "\n",
        "task_2 = {\n",
        "    \"Contributors\": [CONTRIBUTOR],\n",
        "    \"Source\": [SOURCE],\n",
        "    \"Categories\": [\"Question Generation\"],\n",
        "    \"Definition\": definition_2,\n",
        "    \"Positive Examples\": positive_examples_2,\n",
        "    \"Negative Examples\": negative_examples_2,\n",
        "    \"Instances\": topic_questions_instances_cleaned\n",
        "}\n",
        "\n",
        "with open(\"task146_com_qa_question_generation.json\", 'w') as fout:\n",
        "    json_dumps_str = json.dumps(task_2, indent=4)\n",
        "    print(json_dumps_str, file=fout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl4TmBWTzj9d"
      },
      "source": [
        "# Task 3\n",
        "\n",
        "definition_3 = \"Given a group of questions that mean the same thing, answer the question. The answer should exactly answer all the questions given without any ambiguity. Don't give related answers or partial answers.\"\n",
        "\n",
        "positive_examples_3 = [{\n",
        "    \"input\": \"Questions: ['what is the amount of us representatives of nevada?', 'what are the us representative number for nevada?','what is the number of us representatives in nevada?']\",\n",
        "    \"output\": \"4\",\n",
        "    \"explanation\": \"The output answer is the exact answer for the given questions\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Questions: ['when did bill clinton and hillary clinton married?', 'when did bill clinton get married to hillary rodham clinton?', 'what year did bill clinton marry hillary rodham clinton?', 'when did bill clinton and hillary clinton get married?', 'when did hillary clinton marry bill?', 'what date did bill clinton and hillary clinton get married?']\",\n",
        "    \"output\": \"1975-10-11\",\n",
        "    \"explanation\": \"All the questions are answered by the generated answer without any ambiguity\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Questions: ['what year did apollo 11 first land on the moon?', 'what year did apollo 11's first astronauts land on the moon?', 'what year the apollo 11 arrived for the first time into the moon?']\",\n",
        "    \"output\": \"1969\",\n",
        "    \"explanation\": \"The questions are directly answered by the output answer\"\n",
        "}]\n",
        "\n",
        "negative_examples_3 = [{\n",
        "    \"input\": \"Questions:['What is the name of the Formula 1 Circuit in India?', 'Which circuit hosted the first Indian Grand Prix?'] \",\n",
        "    \"output\": \"Delhi\",\n",
        "    \"explanation\": \"The output answer doesn't correctly answer the give questions even though the exact answer is related to the output answer.\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Questions: ['What pigment is used by leaves for generating energy?', 'What gives leaves it's green color?']\",\n",
        "    \"output\": \"Photosynthesis\",\n",
        "    \"explanation\": \"The questions are not directly answered by the answer\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Questions: ['What is the lastest version of macOS called?']\",\n",
        "    \"output\": \"Apple\",\n",
        "    \"explanation\": \"The question and the answer are indrectly related but is not an exact answer to the given question\"\n",
        "}]\n",
        "\n",
        "temp_set = set()\n",
        "daq_cleaned = []\n",
        "for instance in direct_answer_questions:\n",
        "  if instance['input'] not in temp_set:\n",
        "    temp_set.add(instance['input'])\n",
        "    daq_cleaned.append(instance)\n",
        "\n",
        "task_3 = {\n",
        "    \"Contributors\": [CONTRIBUTOR],\n",
        "    \"Source\": [SOURCE],\n",
        "    \"Categories\": [\"Answer Generation\"],\n",
        "    \"Definition\": definition_3,\n",
        "    \"Positive Examples\": positive_examples_3,\n",
        "    \"Negative Examples\": negative_examples_3,\n",
        "    \"Instances\": daq_cleaned\n",
        "}\n",
        "\n",
        "with open(\"task147_com_qa_answer_generation.json\", 'w') as fout:\n",
        "    json_dumps_str = json.dumps(task_3, indent=4)\n",
        "    print(json_dumps_str, file=fout)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg_lENnJzmSY"
      },
      "source": [
        "# Task 4\n",
        "\n",
        "definition_4 = \"Given a set of questions, give a Wikipedia page link that might answer all the questions. The Wikipedia page should be directly related to the set of questions\"\n",
        "\n",
        "positive_examples_4 = [{\n",
        "    \"input\": \"Questions: ['which american president held office for the shortest amount of time?', 'which president held office for the shortest period of time?', 'which person in the us held office for the shortest amount of time?', 'which us president ruled for the shortest time?', 'what president of the united states held office for the shortest period?', 'what us president stayed in office for the shortest amount of time?']\",\n",
        "    \"output\": \"https://en.wikipedia.org/wiki/william_henry_harrison\",\n",
        "    \"explanation\": \"The wikipedia page is related to all the questions and is also an direct answer to all the questions.\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Questions: ['who was the first female to be on a wheaties box?', 'first female olympic athlete on wheaties box?', 'who was the first female athlete to be on the wheaties box?']\",\n",
        "    \"output\": \"https://en.wikipedia.org/wiki/mary_lou_retton\",\n",
        "    \"explanation\": \"The given link contains information about the exact answer of the all questions.\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Questions: ['what is the fourth biggest city in scotland?']\",\n",
        "    \"output\": \"https://en.wikipedia.org/wiki/dundee\",\n",
        "    \"explanation\": \"The linked wikipedia page is directly related to the given question\"\n",
        "}]\n",
        "\n",
        "negative_examples_4 = [{\n",
        "    \"input\": \"Questions: ['What is the name of the Internationa airport in Phoenix?', 'Name any one of Arizona's international airports']\",\n",
        "    \"output\": \"https://en.wikipedia.org/wiki/Arizona_State_University\",\n",
        "    \"explanation\": \"The wikipedia page is not directly related to the given questions even though the answer might be present in the page\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Questions: ['How did World War I start?', 'Who started world war 1?']\",\n",
        "    \"output\": \"https://en.wikipedia.org/wiki/World_War_II\",\n",
        "    \"explanation\": \"The given link does not contain any relevant information about the questions asked\"\n",
        "},\n",
        "{\n",
        "    \"input\": \"Questions: ['What is the largest city by population?', 'Which city has the highest population?']\",\n",
        "    \"output\": \"https://en.wikipedia.org/wiki/Chongqing\",\n",
        "    \"explanation\": \"The linked wikipedia page is not the correct answer for the given questions.\"\n",
        "}]\n",
        "\n",
        "temp_set = set()\n",
        "wl_cleaned = []\n",
        "for instance in wikilink_questions_instances:\n",
        "  if instance['input'] not in temp_set:\n",
        "    temp_set.add(instance['input'])\n",
        "    wl_cleaned.append(instance)\n",
        "\n",
        "task_4 = {\n",
        "    \"Contributors\": [CONTRIBUTOR],\n",
        "    \"Source\": [SOURCE],\n",
        "    \"Categories\": [\"Answer Generation\"],\n",
        "    \"Definition\": definition_4,\n",
        "    \"Positive Examples\": positive_examples_4,\n",
        "    \"Negative Examples\": negative_examples_4,\n",
        "    \"Instances\": wl_cleaned\n",
        "}\n",
        "\n",
        "with open(\"task148_com_qa_answer_generation.json\", 'w') as fout:\n",
        "    json_dumps_str = json.dumps(task_4, indent=4)\n",
        "    print(json_dumps_str, file=fout)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}